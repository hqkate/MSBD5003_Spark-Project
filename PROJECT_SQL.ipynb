{"nbformat_minor": 1, "cells": [{"execution_count": 125, "cell_type": "code", "source": "from pyspark.ml.feature import MinMaxScaler\nfrom pyspark.ml.linalg import Vectors, VectorUDT\n\ndataframe = spark.read.csv('wasb:///creditcard.csv', header=True, inferSchema=True)\ndataframe.cache()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "DataFrame[Time: decimal(10,0), V1: double, V2: double, V3: double, V4: double, V5: double, V6: double, V7: double, V8: double, V9: double, V10: double, V11: double, V12: double, V13: double, V14: double, V15: double, V16: double, V17: double, V18: double, V19: double, V20: double, V21: double, V22: double, V23: double, V24: double, V25: double, V26: double, V27: double, V28: double, Amount: double, Class: int]"}], "metadata": {"collapsed": false}}, {"execution_count": 43, "cell_type": "code", "source": "#dataframe.show()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 44, "cell_type": "code", "source": "#dataframe.printSchema()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": "dataframe.groupBy('Class').count().show()\n# this shows that the dataset is imbalanced ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+------+\n|Class| count|\n+-----+------+\n|    1|   492|\n|    0|284315|\n+-----+------+\n\n"}], "metadata": {"collapsed": false}}, {"execution_count": 87, "cell_type": "code", "source": "from pyspark.sql.functions import udf, log, lit\n# Data Transfromation on Amount with formula log(Amount+1)\ndataframe = dataframe.select('*',(log(dataframe.Amount + 1)).alias('logAmount'))\namount = dataframe.select('logAmount')\namount.show()\namount.cache()\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------------------+\n|         logAmount|\n+------------------+\n| 5.014760108673205|\n|1.3056264580524357|\n| 5.939276115362396|\n| 4.824305715904762|\n| 4.262539022051294|\n| 1.541159071680806|\n|1.7900914121273581|\n|3.7328963395307104|\n| 4.545420181582317|\n|1.5432981099295553|\n| 2.174751721484161|\n|  2.39698576841553|\n| 4.808111029984782|\n| 3.349904087274605|\n| 4.091005660956586|\n|2.8326249356838407|\n|  2.63834278867739|\n|0.6365768290715511|\n|  3.86702563949741|\n| 1.791759469228055|\n+------------------+\nonly showing top 20 rows\n\nDataFrame[logAmount: double]"}], "metadata": {"scrolled": true, "collapsed": false}}, {"execution_count": 88, "cell_type": "code", "source": "slen = udf(lambda vs: Vectors.dense(vs), VectorUDT())\n# amountNormalized.select('*', slen(amountNormalized['Amount']).alias('slen')).show()\ndataframe_with = dataframe.withColumn('slen', slen(amount['logAmount']))\ndataframe_with.cache()\n# dataframe_with.show()\nmmScaler = MinMaxScaler(inputCol=\"slen\", outputCol=\"scaled\")\nmodel = mmScaler.fit(dataframe_with)\n# print mmScaler.explainParams()\nnormalized_df = model.transform(dataframe_with)\nnormalized_df.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------+-----+------------------+--------------------+--------------------+\n|Time|          V1|          V2|          V3|          V4|          V5|          V6|          V7|          V8|          V9|         V10|         V11|         V12|         V13|         V14|         V15|         V16|         V17|         V18|         V19|         V20|         V21|         V22|         V23|         V24|         V25|         V26|         V27|         V28|Amount|Class|         logAmount|                slen|              scaled|\n+----+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------+-----+------------------+--------------------+--------------------+\n|   0|-1.359807134|-0.072781173| 2.536346738| 1.378155224| -0.33832077| 0.462387778| 0.239598554| 0.098697901|  0.36378697| 0.090794172|-0.551599533|-0.617800856|-0.991389847|-0.311169354| 1.468176972|-0.470400525| 0.207971242|  0.02579058|  0.40399296| 0.251412098|-0.018306778| 0.277837576| -0.11047391| 0.066928075| 0.128539358|-0.189114844| 0.133558377|-0.021053053|149.62|    0| 5.014760108673205| [5.014760108673205]|[0.49387326819224...|\n|   0| 1.191857111| 0.266150712| 0.166480113| 0.448154078| 0.060017649|-0.082360809|-0.078802983| 0.085101655|-0.255425128|-0.166974414| 1.612726661| 1.065235311| 0.489095016|-0.143772296| 0.635558093| 0.463917041|-0.114804663| -0.18336127|-0.145783041|-0.069083135|-0.225775248|-0.638671953| 0.101288021|-0.339846476| 0.167170404| 0.125894532|-0.008983099| 0.014724169|  2.69|    0|1.3056264580524357|[1.3056264580524357]|[0.1285832207130687]|\n|   1|-1.358354062|-1.340163075| 1.773209343| 0.379779593|-0.503198133| 1.800499381| 0.791460956| 0.247675787|-1.514654323| 0.207642865| 0.624501459| 0.066083685| 0.717292731|-0.165945923| 2.345864949|-2.890083194| 1.109969379|-0.121359313|-2.261857095| 0.524979725| 0.247998153| 0.771679402| 0.909412262|-0.689280956|-0.327641834|-0.139096572|-0.055352794|-0.059751841|378.66|    0| 5.939276115362396| [5.939276115362396]|[0.5849232350550524]|\n|   1|-0.966271712|-0.185226008|  1.79299334|-0.863291275| -0.01030888| 1.247203168|  0.23760894| 0.377435875|-1.387024063|-0.054951922|-0.226487264| 0.178228226|  0.50775687|-0.287923745|-0.631418118|-1.059647245|-0.684092786| 1.965775003| -1.23262197|-0.208037781|-0.108300452| 0.005273597|-0.190320519|-1.175575332| 0.647376035|-0.221928844| 0.062722849| 0.061457629| 123.5|    0| 4.824305715904762| [4.824305715904762]|[0.4751165716883687]|\n|   2|-1.158233093| 0.877736755| 1.548717847| 0.403033934|-0.407193377| 0.095921462| 0.592940745|-0.270532677| 0.817739308| 0.753074432|-0.822842878|  0.53819555| 1.345851593|-1.119669835|  0.17512113|-0.451449183|-0.237033239|-0.038194787| 0.803486925|  0.40854236|-0.009430697| 0.798278495| -0.13745808| 0.141266984|-0.206009588| 0.502292224|  0.21942223| 0.215153147| 69.99|    0| 4.262539022051294| [4.262539022051294]|[0.4197915816504368]|\n|   2|-0.425965884| 0.960523045| 1.141109342| -0.16825208| 0.420986881|-0.029727552| 0.476200949| 0.260314333|-0.568671376|-0.371407197|  1.34126198| 0.359893837|-0.358090653|  -0.1371337| 0.517616807| 0.401725896|-0.058132823| 0.068653149|-0.033193788| 0.084967672|-0.208253515|-0.559824796|-0.026397668|-0.371426583|-0.232793817| 0.105914779| 0.253844225| 0.081080257|  3.67|    0| 1.541159071680806| [1.541159071680806]|[0.15177939742694...|\n|   4| 1.229657635| 0.141003507| 0.045370774| 1.202612737| 0.191880989| 0.272708123|-0.005159003|  0.08121294| 0.464959995|-0.099254321|-1.416907243|-0.153825826|-0.751062716| 0.167371963| 0.050143594|-0.443586798| 0.002820512| -0.61198734|-0.045575045|-0.219632553|-0.167716266|-0.270709726|-0.154103787|-0.780055415| 0.750136936|-0.257236846|  0.03450743| 0.005167769|  4.99|    0|1.7900914121273581|[1.7900914121273581]|[0.17629523185788...|\n|   7|-0.644269442| 1.417963545| 1.074380376|-0.492199018| 0.948934095| 0.428118463| 1.120631358|-3.807864239| 0.615374731| 1.249376178|-0.619467796| 0.291474353| 1.757964214| -1.32386522| 0.686132504|-0.076126999|-1.222127345| -0.35822157| 0.324504731|-0.156741852|  1.94346534| -1.01545471|  0.05750353|-0.649709006|-0.415266566|-0.051634297|-1.206921081|-1.085339188|  40.8|    0|3.7328963395307104|[3.7328963395307104]| [0.367630290397812]|\n|   7|-0.894286082| 0.286157196|-0.113192213| -0.27152613|  2.66959866| 3.721818061| 0.370145128| 0.851084443|-0.392047587|-0.410430433|-0.705116587|-0.110452262|-0.286253632|  0.07435536| -0.32878305|-0.210077268|-0.499767969| 0.118764861| 0.570328167| 0.052735669|  -0.0734251|-0.268091632| -0.20423267| 1.011591802|  0.37320468|-0.384157308| 0.011747356|  0.14240433|  93.2|    0| 4.545420181582317| [4.545420181582317]|[0.44765082909997...|\n|   9|-0.338261752| 1.119593376| 1.044366552|-0.222187277| 0.499360806|-0.246761101| 0.651583206| 0.069538587|-0.736727316|-0.366845639| 1.017614468|  0.83638957| 1.006843514|-0.443522817| 0.150219101| 0.739452777|-0.540979922|  0.47667726| 0.451772964| 0.203711455|-0.246913937|-0.633752642|-0.120794084|-0.385049925|-0.069733046| 0.094198834| 0.246219305| 0.083075649|  3.68|    0|1.5432981099295553|[1.5432981099295553]|[0.15199005831357...|\n|  10| 1.449043781|-1.176338825| 0.913859833|-1.375666655|-1.971383165|-0.629152139|-1.423235601| 0.048455888|-1.720408393| 1.626659058|  1.19964395|-0.671439778|-0.513947153|-0.095045045| 0.230930409| 0.031967467| 0.253414716| 0.854343814|-0.221365414|-0.387226474|-0.009301897| 0.313894411| 0.027740158| 0.500512287| 0.251367359|-0.129477954| 0.042849871| 0.016253262|   7.8|    0| 2.174751721484161| [2.174751721484161]|[0.21417808966344...|\n|  10| 0.384978215| 0.616109459|-0.874299703|-0.094018626| 2.924584378| 3.317027168| 0.470454672| 0.538247228|-0.558894612| 0.309755394|-0.259115564|-0.326143234|-0.090046723| 0.362832369| 0.928903661|-0.129486811|-0.809978926|  0.35998539| 0.707663826| 0.125991576| 0.049923686| 0.238421512| 0.009129869|  0.99671021|-0.767314827|-0.492208295| 0.042472442|-0.054337388|  9.99|    0|  2.39698576841553|  [2.39698576841553]|[0.23606457130625...|\n|  10| 1.249998742|-1.221636809| 0.383930151|-1.234898688|-1.485419474|-0.753230165|-0.689404975|-0.227487228|-2.094010573| 1.323729274| 0.227666231|-0.242681999| 1.205416808|-0.317630527|  0.72567499|-0.815612186| 0.873936448|-0.847788599|-0.683192626|-0.102755942|-0.231809239| -0.48328533| 0.084667691| 0.392830885| 0.161134554| -0.35499004| 0.026415549| 0.042422089| 121.5|    0| 4.808111029984782| [4.808111029984782]|[0.4735216554233186]|\n|  11| 1.069373588| 0.287722129| 0.828612727|  2.71252043|-0.178398016|  0.33754373|-0.096716862| 0.115981736|-0.221082566| 0.460230444|-0.773656931| 0.323387245|-0.011075887|-0.178485175|-0.655564278|-0.199925171| 0.124005415|-0.980496202|-0.982916082|-0.153197231|-0.036875532| 0.074412403|-0.071407433| 0.104743753| 0.548264725| 0.104094153| 0.021491058| 0.021293311|  27.5|    0| 3.349904087274605| [3.349904087274605]|[0.32991170940589...|\n|  12|-2.791854766|-0.327770757| 1.641750161| 1.767472744|-0.136588446| 0.807596468| -0.42291139|-1.907107476| 0.755712908| 1.151086988| 0.844555471| 0.792943952| 0.370448093|-0.734975106|  0.40679571|-0.303057624|-0.155868715| 0.778265457| 2.221868014|-1.582122044| 1.151663048| 0.222181966| 1.020586204| 0.028316651|-0.232746324|-0.235557218|-0.164777512|-0.030153637|  58.8|    0| 4.091005660956586| [4.091005660956586]|[0.40289830264765...|\n|  12|-0.752417043| 0.345485415| 2.057322913|-1.468643298| -1.15839368|-0.077849829|-0.608581418| 0.003603484|-0.436166984| 0.747730827|-0.793980603|-0.770406729| 1.047626997|-1.066603681| 1.106953457| 1.660113557|-0.279265373|-0.419994141| 0.432535349| 0.263450864| 0.499624955| 1.353650486| -0.25657328|-0.065083708|-0.039124354|-0.087086473|  -0.1809975| 0.129394059| 15.99|    0|2.8326249356838407|[2.8326249356838407]|[0.2789680272301509]|\n|  12| 1.103215435|-0.040296215| 1.267332089|  1.28909147|-0.735997164| 0.288069163|-0.586056786| 0.189379714| 0.782332892|-0.267975067| -0.45031128| 0.936707715| 0.708380406|-0.468647288| 0.354574063|-0.246634656|-0.009212378|-0.595912406|-0.575681622|-0.113910177|-0.024612006| 0.196001953| 0.013801654| 0.103758331| 0.364297541|-0.382260574| 0.092809187| 0.037050517| 12.99|    0|  2.63834278867739|  [2.63834278867739]|[0.2598343584575347]|\n|  13|-0.436905071| 0.918966213| 0.924590774|-0.727219054| 0.915678718|-0.127867352| 0.707641607| 0.087962355|-0.665271354|-0.737979824| 0.324097813| 0.277192107| 0.252624256| -0.29189646|-0.184520169| 1.143173707|-0.928709263| 0.680469593| 0.025436462|-0.047021282|-0.194795824|-0.672637997|-0.156857514|-0.888386321|-0.342413219|-0.049026729| 0.079692399| 0.131023789|  0.89|    0|0.6365768290715511|[0.6365768290715511]|[0.06269258592953...|\n|  14|-5.401257663|-5.450147834| 1.186304631|   1.7362388| 3.049105878|-1.763405574|-1.559737699| 0.160841747|  1.23308974| 0.345172827| 0.917229868| 0.970116716|-0.266567765|-0.479129929|-0.526608503| 0.472004112|-0.725480945| 0.075081352|-0.406866573|-2.196848025|-0.503600329| 0.984459786| 2.458588576| 0.042118897|-0.481630824|-0.621272014|  0.39205329| 0.949594246|  46.8|    0|  3.86702563949741|  [3.86702563949741]|[0.3808398705769959]|\n|  15| 1.492935977|-1.029345732| 0.454794734| -1.43802588|-1.555434101|-0.720961147| -1.08066413|-0.053127118|-1.978681595| 1.638076037| 1.077542412|-0.632046515|-0.416957167| 0.052010515|-0.042978923|-0.166432496| 0.304241419| 0.554432499| 0.054229515|-0.387910173|-0.177649846|-0.175073809| 0.040002219| 0.295813863| 0.332930599|-0.220384851| 0.022298436| 0.007602256|   5.0|    0| 1.791759469228055| [1.791759469228055]|[0.1764595086715293]|\n+----+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------+-----+------------------+--------------------+--------------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 80, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 95, "cell_type": "code", "source": "# modelling\n\n### Logistic Regression\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\n# training data\nkeep_list = ['Class','Time','V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16','V17','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28','scaled']\nfeature_list = ['Time','V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16','V17','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28','scaled']\ndf_train = normalized_df.select(*keep_list).withColumnRenamed('Class', 'label')\n\ncombineFeature = VectorAssembler(inputCols = feature_list,\n                                 outputCol = \"features\")\n\ndata = combineFeature.transform(df_train)\n\nlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n\n# Fit the model\nlrModel = lr.fit(data)\n\n# Print the coefficients and intercept for logistic regression\nprint(\"Coefficients: \" + str(lrModel.coefficients))\nprint(\"Intercept: \" + str(lrModel.intercept))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Coefficients: (31,[],[])\nIntercept: -6.35935934092"}], "metadata": {"collapsed": false}}, {"execution_count": 93, "cell_type": "code", "source": "data.select('features').show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+\n|            features|\n+--------------------+\n|[0.0,-1.359807134...|\n|[0.0,1.191857111,...|\n|[1.0,-1.358354062...|\n|[1.0,-0.966271712...|\n|[2.0,-1.158233093...|\n|[2.0,-0.425965884...|\n|[4.0,1.229657635,...|\n|[7.0,-0.644269442...|\n|[7.0,-0.894286082...|\n|[9.0,-0.338261752...|\n|[10.0,1.449043781...|\n|[10.0,0.384978215...|\n|[10.0,1.249998742...|\n|[11.0,1.069373588...|\n|[12.0,-2.79185476...|\n|[12.0,-0.75241704...|\n|[12.0,1.103215435...|\n|[13.0,-0.43690507...|\n|[14.0,-5.40125766...|\n|[15.0,1.492935977...|\n+--------------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 121, "cell_type": "code", "source": "### Random forest classifier\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+------------+--------------------+\n|prediction|indexedLabel|            features|\n+----------+------------+--------------------+\n|       0.0|         0.0|[1.0,-1.358354062...|\n|       0.0|         0.0|[1.0,-0.966271712...|\n|       0.0|         0.0|[2.0,-1.158233093...|\n|       0.0|         0.0|[4.0,1.229657635,...|\n|       0.0|         0.0|[10.0,0.384978215...|\n+----------+------------+--------------------+\nonly showing top 5 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 122, "cell_type": "code", "source": "# Cross Validation\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(lr.regParam, [0.1, 0.01]) \\\n    .build()\n    \nevaluator = BinaryClassificationEvaluator()\nevaluator.setLabelCol(\"indexedLabel\")\nevaluator.setRawPredictionCol(\"prediction\")\n\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=3)  \n\n# Run cross-validation, and choose the best set of parameters.\ncvModel = crossval.fit(trainingData)\n\npredictionsDf = cvModel.transform(testData)\n\nnumSuccesses = predictionsDf.where('label == prediction').count()\nnumInspections = predictionsDf.count()\n\nprint (\"There were %d inspections and there were %d successful predictions\" % (numInspections, numSuccesses))\nprint(\"This is a %f%% success rate\" % (float(numSuccesses) / float(numInspections) * 100))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "There were 85340 inspections and there were 85287 successful predictions\nThis is a 99.937895% success rate\nException AttributeError: \"'BinaryClassificationEvaluator' object has no attribute '_java_obj'\" in <object repr() failed> ignored"}], "metadata": {"collapsed": false}}, {"execution_count": 123, "cell_type": "code", "source": "### Gradient-boosted tree classifier\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\n# Train a GBT model.\ngbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)\n\n# Chain indexers and GBT in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\ngbtModel = model.stages[2]\nprint(gbtModel)  # summary only\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+------------+--------------------+\n|prediction|indexedLabel|            features|\n+----------+------------+--------------------+\n|       0.0|         0.0|[0.0,-1.359807134...|\n|       0.0|         0.0|[0.0,1.191857111,...|\n|       0.0|         0.0|[1.0,-0.966271712...|\n|       0.0|         0.0|[4.0,1.229657635,...|\n|       0.0|         0.0|[7.0,-0.894286082...|\n+----------+------------+--------------------+\nonly showing top 5 rows\n\nTest Error = 0.000679188\nGBTClassificationModel (uid=GBTClassifier_48be906c2c939c1e4b78) with 10 trees"}], "metadata": {"collapsed": false}}, {"execution_count": 124, "cell_type": "code", "source": "# Cross Validation\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(lr.regParam, [0.1, 0.01]) \\\n    .build()\n    \nevaluator = BinaryClassificationEvaluator()\nevaluator.setLabelCol(\"indexedLabel\")\nevaluator.setRawPredictionCol(\"prediction\")\n\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=3)  \n\n# Run cross-validation, and choose the best set of parameters.\ncvModel = crossval.fit(trainingData)\n\npredictionsDf = cvModel.transform(testData)\n\nnumSuccesses = predictionsDf.where('label == prediction').count()\nnumInspections = predictionsDf.count()\n\nprint (\"There were %d inspections and there were %d successful predictions\" % (numInspections, numSuccesses))\nprint(\"This is a %f%% success rate\" % (float(numSuccesses) / float(numInspections) * 100))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "There were 84948 inspections and there were 84893 successful predictions\nThis is a 99.935255% success rate"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}, "anaconda-cloud": {}}}