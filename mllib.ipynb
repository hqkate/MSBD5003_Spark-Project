{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1490086759585_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-yikecl.fcmrfbizykpurkxecvqvc5w13h.hx.internal.cloudapp.net:8088/proxy/application_1490086759585_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.6:30060/node/containerlogs/container_1490086759585_0007_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic example on Transformer and Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare training data from a list of (label, features) tuples.\n",
    "# Dense Vectors are just NumPy arrays\n",
    "\n",
    "training = spark.createDataFrame([\n",
    "    (1, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n",
    "# Create a LogisticRegression instance. This instance is an Estimator.\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "\n",
    "# Print out the parameters, documentation, and any default values.\n",
    "print lr.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
    "model1 = lr.fit(training)\n",
    "\n",
    "# model1 is a Model (i.e., a transformer produced by an Estimator)\n",
    "print \"Model 1's trained coefficients: \", model1.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We may alternatively specify parameters using a Python dictionary as a paramMap\n",
    "paramMap = {lr.maxIter: 20}\n",
    "paramMap[lr.maxIter] = 30  # Specify 1 Param, overwriting the original maxIter.\n",
    "paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55})  # Specify multiple Params.\n",
    "\n",
    "# You can combine paramMaps, which are python dictionaries.\n",
    "paramMap[lr.probabilityCol] = \"myProbability\"  # Change output column name\n",
    "\n",
    "# Now learn a new model using the paramMapCombined parameters.\n",
    "# paramMapCombined overrides all parameters set earlier via lr.set* methods.\n",
    "model2 = lr.fit(training, paramMap)\n",
    "print \"Model 2's trained coefficients: \", model2.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "test = spark.createDataFrame([\n",
    "    (1, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (2, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (3, Vectors.dense([0.0, 2.2, -1.5]))], [\"id\", \"features\"])\n",
    "\n",
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "# LogisticRegression.transform will only use the 'features' column.\n",
    "# Note that model2.transform() outputs a \"myProbability\" column instead of the usual\n",
    "# 'probability' column since we renamed the lr.probabilityCol parameter previously.\n",
    "\n",
    "model1.transform(test).show()\n",
    "model2.transform(test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipline example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d spark spark\", 1),\n",
    "    (1, \"b d\", 0),\n",
    "    (2, \"spark f g h\", 1),\n",
    "    (3, \"hadoop mapreduce\", 0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A tokenizer converts the input string to lowercase and then splits it by white spaces.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tokenizer.transform(training).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The same can be achieved by DataFrameAPI:\n",
    "# But you will need to wrap it as a transformer to use it in a pipeline.\n",
    "\n",
    "training.select('*', split(training['text'],' ').alias('words')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Maps a sequence of terms to their term frequencies using the hashing trick.\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "a = hashingTF.transform(tokenizer.transform(training))\n",
    "a.show(truncate=False)\n",
    "\n",
    "print a.select('features').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lr is an estimator\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "# Now we are ready to assumble the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "                            \n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "model.transform(test).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example showing a DAG pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "# Using two different hash functions to turn the words into vectors\n",
    "hashingTF1 = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"feature1\")\n",
    "hashingTF2 = HashingTF(numFeatures = 1 << 10,\n",
    "                       inputCol=tokenizer.getOutputCol(), outputCol=\"feature2\")\n",
    "\n",
    "# Combine two vectors into one.  VectorAssember is an transformer\n",
    "combineFeature = VectorAssembler(inputCols=[\"feature1\", \"feature2\"],\n",
    "                                 outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "# Stages must be in topological order\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF1, hashingTF2, combineFeature, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "model.transform(test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## Example: Analyzing food inspection data using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inspections = spark.read.csv('wasb://cluster@msbd.blob.core.windows.net/HdiSamples/HdiSamples/FoodInspectionData/Food_Inspections1.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at its schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inspections.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inspections.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the CSV file as a DataFrame. It has some columns we will not use. Dropping them can save memory when caching the DataFrame. Also, we should give these columns meaningful names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop unused columns and rename interesting columns.\n",
    "\n",
    "# Keep interesting columns and rename them to something meaningful\n",
    "\n",
    "# Mapping column index to name.\n",
    "columnNames = {0: \"id\", 1: \"name\", 12: \"results\", 13: \"violations\"}\n",
    "    \n",
    "# Rename column from '_c{id}' to something meaningful.\n",
    "cols = [inspections[i].alias(columnNames[i]) for i in columnNames.keys()]\n",
    "   \n",
    "# Drop columns we are not using.\n",
    "df = inspections.select(cols).where(col('violations').isNotNull())\n",
    "\n",
    "df.cache()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the above cell gives us an idea of the schema of the input file; the file includes the name of every establishment, the type of establishment, the address, the data of the inspections, and the location, among other things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start to get a sense of what our dataset contains. For example, what are the different values in the `results` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.select('results').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupBy('results').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us develop a model that can guess the outcome of a food inspection, given the violations. Since logistic regression is a binary classification method, it makes sense to group our data into two categories: **Fail** and **Pass**. A \"Pass w/ Conditions\" is still a Pass, so when we train the model, we will consider the two results equivalent. Data with the other results (\"Business Not Located\", \"Out of Business\") are not useful so we will remove them from our training set. This should be okay since these two categories make up a very small percentage of the results anyway.\n",
    "\n",
    "Let us go ahead and convert our existing dataframe(`df`) into a new dataframe where each inspection is represented as a label-violations pair. In our case, a label of `0.0` represents a failure, a label of `1.0` represents a success, and a label of `-1.0` represents some results besides those two. We will filter those other results out when computing the new data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The function to clean the data\n",
    "\n",
    "labeledData = df.select(when(df.results == 'Fail', 0)\n",
    "                        .when(df.results == 'Pass', 1)\n",
    "                        .when(df.results == 'Pass w/ Conditions', 1)\n",
    "                        .alias('label'), \n",
    "                        'violations') \\\n",
    "                .where('label >= 0')\n",
    "\n",
    "labeledData = cleanData(df)\n",
    "    \n",
    "labeledData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression model from the input dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainingData, testData = labeledData.randomSplit([0.8, 0.2])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"violations\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "predictionsDf = model.transform(testData)\n",
    "predictionsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numSuccesses = predictionsDf.where('label == prediction').count()\n",
    "numInspections = predictionsDf.count()\n",
    "\n",
    "print (\"There were %d inspections and there were %d successful predictions\" % (numInspections, numSuccesses))\n",
    "print(\"This is a %d%% success rate\" % (float(numSuccesses) / float(numInspections) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "CrossValidator begins by splitting the dataset into a set of folds which are used as separate training and test datasets. E.g., with k=3 folds, CrossValidator will generate 3 (training, test) dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing. To evaluate a particular ParamMap, CrossValidator computes the average evaluation metric for the 3 Models produced by fitting the Estimator on the 3 different (training, test) dataset pairs.\n",
    "\n",
    "After identifying the best ParamMap, CrossValidator finally re-fits the Estimator using the best ParamMap and the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)  \n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(trainingData)\n",
    "\n",
    "predictionsDf = cvModel.transform(testData)\n",
    "\n",
    "numSuccesses = predictionsDf.where('label == prediction').count()\n",
    "numInspections = predictionsDf.count()\n",
    "\n",
    "print (\"There were %d inspections and there were %d successful predictions\" % (numInspections, numSuccesses))\n",
    "print(\"This is a %d%% success rate\" % (float(numSuccesses) / float(numInspections) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvModel.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  },
  "widgets": {
   "state": {
    "6e9e93a4c6b04588a79d6ee36f392422": {
     "views": [
      {
       "cell_index": 21
      }
     ]
    },
    "cd9a9b22fd1641b7a624fba5271e8b86": {
     "views": [
      {
       "cell_index": 21
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}