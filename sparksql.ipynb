{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1489483935825_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-yikecl.bkfsu5nbv1tu1ewb0bzierqo3d.hx.internal.cloudapp.net:8088/proxy/application_1489483935825_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.16:30060/node/containerlogs/container_1489483935825_0006_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "Row(age=11, name='Alice')\n",
      "Alice 11\n",
      "Alice 11\n",
      "<built-in method count of Row object at 0x7f9a96a0fb90>\n",
      "1"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row = Row(name=\"Alice\", age=11)\n",
    "print row\n",
    "print row['name'], row['age']\n",
    "print row.name, row.age\n",
    "\n",
    "row = Row(name=\"Alice\", age=11, count=1)\n",
    "print row.count\n",
    "print row['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('wasb:///HdiSamples/HdiSamples/SensorSampleData/building/building.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show the content of the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the dataframe schema in a tree format\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an RDD from the dataframe\n",
    "dfrdd = df.rdd\n",
    "dfrdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve specific columns from the dataframe\n",
    "df.select('BuildingID', 'Country').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.filter(\"Country='USA'\").select('*', lit('OK').alias('Status')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use GroupBy clause with dataframe \n",
    "df.groupBy('HVACProduct').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriting SQL with DataFrame API\n",
    "\n",
    "The data files have been put to a public blob container, which can be accessed as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data from csv files\n",
    "\n",
    "dfCustomer = spark.read.csv('wasb://cluster@msbd.blob.core.windows.net/data/Customer.csv', header=True, inferSchema=True)\n",
    "dfProduct = spark.read.csv('wasb://cluster@msbd.blob.core.windows.net/data/Product.csv', header=True, inferSchema=True)\n",
    "dfDetail = spark.read.csv('wasb://cluster@msbd.blob.core.windows.net/data/SalesOrderDetail.csv', header=True, inferSchema=True)\n",
    "dfHeader = spark.read.csv('wasb://cluster@msbd.blob.core.windows.net/data/SalesOrderHeader.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SELECT ProductID, Name, ListPrice \n",
    "# FROM Product \n",
    "# WHERE Color = 'black'\n",
    "\n",
    "dfProduct.filter(\"Color = 'Black'\")\\\n",
    "         .select('ProductID', 'Name', 'ListPrice')\\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfProduct.where(dfProduct.Color=='Black') \\\n",
    "         .select(dfProduct.ProductID, dfProduct['Name'], dfProduct.ListPrice * 2) \\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfProduct.where(\"ListPrice * 2 > 100\") \\\n",
    "         .select(dfProduct.ProductID, dfProduct['Name'], dfProduct.ListPrice * 2) \\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SELECT ProductID, Name, ListPrice \n",
    "# FROM Product \n",
    "# WHERE Color = 'black' \n",
    "# ORDER BY ProductID\n",
    "\n",
    "dfProduct.filter(\"Color = 'Black'\")\\\n",
    "         .select('ProductID', 'Name', 'ListPrice')\\\n",
    "         .orderBy('ListPrice')\\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find all orders and details on black product,\n",
    "# return the product SalesOrderID, SalesOrderDetailID, Name, UnitPrice, and OrderQty\n",
    "\n",
    "# SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty \n",
    "# FROM SalesLT.SalesOrderDetail, SalesLT.Product\n",
    "# WHERE SalesOrderDetail.ProductID = Product.ProductID AND Color = 'Black'\n",
    "\n",
    "# SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty \n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID\n",
    "# WHERE Color = 'Black'\n",
    "\n",
    "# Spark SQL supports natural joins\n",
    "\n",
    "dfDetail.join(dfProduct.filter(\"Color='Black'\"), 'ProductID') \\\n",
    "        .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty') \\\n",
    "        .show()\n",
    "\n",
    "# If we move the filter to after select, it still works.  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This also works:\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d2 = d1.filter(\"Color = 'Black'\")\n",
    "d2.show()\n",
    "# d2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will report an error:\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d1.write.csv('wasb:///temp.csv', mode = 'overwrite', header = True)\n",
    "d2 = spark.read.csv('wasb:///temp.csv', header = True, inferSchema = True)\n",
    "d2.filter(\"Color = 'Black'\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find all orders that include at least one black product, \n",
    "# return the product SalesOrderID, Name, UnitPrice, and OrderQty\n",
    "\n",
    "# SELECT DISTINCT SalesOrderID\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID\n",
    "# WHERE Color = 'Black'\n",
    "\n",
    "dfDetail.join(dfProduct.filter(\"Color='Black'\"), 'ProductID') \\\n",
    "        .select('SalesOrderID') \\\n",
    "        .distinct() \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many colors in the products?\n",
    "\n",
    "# SELECT COUNT(DISTINCT Color)\n",
    "# FROM SalesLT.Product\n",
    "\n",
    "dfProduct.select('Color').distinct().count()\n",
    "\n",
    "# It's 1 more than standard SQL.  In standard SQL, COUNT() does not count NULLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find the total price of each order, \n",
    "# return SalesOrderID and total price (column name should be ‘totalprice’)\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# GROUP BY SalesOrderID\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail.OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the total price of each order where the total price > 10000\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# GROUP BY SalesOrderID\n",
    "# HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .filter('TotalPrice > 10000')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the total price on the black products of each order where the total price > 10000\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail, SalesLT.Product\n",
    "# WHERE SalesLT.SalesOrderDetail.ProductID = SalesLT.Product.ProductID AND Color = 'Black'\n",
    "# GROUP BY SalesOrderID\n",
    "# HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .join(dfProduct, 'ProductID').where(\"Color = 'Black'\") \\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .filter('TotalPrice > 10000')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each customer, find the total quantity of black products bought.\n",
    "# Report CustomerID, FirstName, LastName, and total quantity\n",
    "\n",
    "# select saleslt.customer.customerid, FirstName, LastName, sum(orderqty)\n",
    "# from saleslt.customer\n",
    "# left outer join \n",
    "# (\n",
    "# saleslt.salesorderheader\n",
    "# join saleslt.salesorderdetail\n",
    "# on saleslt.salesorderdetail.salesorderid = saleslt.salesorderheader.salesorderid\n",
    "# join saleslt.product\n",
    "# on saleslt.product.productid = saleslt.salesorderdetail.productid and color = 'black'\n",
    "# )\n",
    "# on saleslt.customer.customerid = saleslt.salesorderheader.customerid\n",
    "# group by saleslt.customer.customerid, FirstName, LastName\n",
    "# order by sum(orderqty) desc\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID')\\\n",
    "             .where('Color = \"Black\"') \\\n",
    "             .join(dfHeader, 'SalesOrderID')\\\n",
    "             .groupBy('CustomerID').sum('OrderQty')\n",
    "dfCustomer.join(d1, 'CustomerID', 'left_outer')\\\n",
    "          .select('CustomerID', 'FirstName', 'LastName', 'sum(OrderQty)')\\\n",
    "          .orderBy('sum(OrderQty)', ascending=False)\\\n",
    "          .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "### Embed SQL queries\n",
    "\n",
    "You can also run SQL queries over dataframes once you register them as temporary tables within the SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Register the dataframe as a temporary view called HVAC\n",
    "df.createOrReplaceTempView('HVAC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.sql('SELECT * FROM HVAC WHERE BuildingAge >= 10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can even mix DataFrame API with SQL:\n",
    "df.where('BuildingAge >= 10').createOrReplaceTempView('OldBuildings')\n",
    "spark.sql('SELECT HVACproduct, COUNT(*) FROM OldBuildings GROUP BY HVACproduct').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d1 = spark.sql('SELECT * FROM HVAC WHERE BuildingAge >= 10')\n",
    "d1.groupBy('HVACproduct').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UDF\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "slen = udf(lambda s: len(s), IntegerType())\n",
    "df.select('*', slen(df['Country']).alias('slen')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.udf.register('slen', lambda s: len(s), IntegerType())\n",
    "spark.sql('SELECT *, slen(Country) AS slen FROM HVAC').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Flexible Data Model\n",
    "\n",
    "Sample data file at\n",
    "\n",
    "https://msbd.blob.core.windows.net/cluster/data/products.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1489453106427_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-yikecl.3lencbmfhhvehpklsy3xivsphf.hx.internal.cloudapp.net:8088/proxy/application_1489453106427_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.6:30060/node/containerlogs/container_1489453106427_0004_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "root\n",
      " |-- dimensions: struct (nullable = true)\n",
      " |    |-- height: double (nullable = true)\n",
      " |    |-- length: double (nullable = true)\n",
      " |    |-- width: double (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- warehouseLocation: struct (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('wasb://cluster@msbd.blob.core.windows.net/data/products.json')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accessing nested fields\n",
    "\n",
    "df.select(df['dimensions.height']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.select('dimensions.height').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.select('dimensions.height')\\\n",
    "  .filter(\"tags[0] = 'cold' AND warehouseLocation.latitude < 0\")\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Converting between RDD and DataFrame\n",
    "\n",
    "Sample data file at:\n",
    "\n",
    "https://msbd.blob.core.windows.net/cluster/data/people.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"wasb://cluster@msbd.blob.core.windows.net/data/people.txt\")\n",
    "\n",
    "def parse(l):\n",
    "    a = l.split(',')\n",
    "    return (a[0], int(a[1]))\n",
    "\n",
    "rdd = lines.map(parse)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the DataFrame from an RDD of tuples, schema is inferred\n",
    "df = spark.createDataFrame(rdd)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the DataFrame from an RDD of tuples with column names, type is inferred\n",
    "df = spark.createDataFrame(rdd, ['name', 'age'])\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the DataFrame from an RDD of Rows, type is given in the Row objects\n",
    "from pyspark.sql import Row\n",
    "\n",
    "rdd_rows = rdd.map(lambda p: Row(name = p[0], age = p[1]))\n",
    "df = spark.createDataFrame(rdd_rows)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Row fields with types incompatible with that of previous rows will be turned into nulls\n",
    "row1 = Row(name=\"Alice\", age=11)\n",
    "row2 = Row(name=\"Bob\", age='12')\n",
    "rdd_rows = sc.parallelize([row1, row2])\n",
    "df1 = spark.createDataFrame(rdd_rows)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rdd returns the content as an RDD of Rows\n",
    "teenagers = df.filter('age >= 13 and age <= 19')\n",
    "\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name)\n",
    "teenNames.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "DataFrames are stored using columnar storage with compression\n",
    "\n",
    "RDDs are stored using row storage without compression\n",
    "\n",
    "The RDD view of DataFrame just provides an interface, the Row objects are constructed on the fly and do not necessarily represent the internal storage format of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closure in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = range(10)\n",
    "df = spark.createDataFrame(zip(data, data))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The 'closure' behaviour in RDD doesn't seem to exist for DataFrames\n",
    "\n",
    "x = 5\n",
    "df1 = df.filter(df._1 < x)\n",
    "print df1.show()\n",
    "x = 3\n",
    "print df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Because of the Catalyst optimizer !\n",
    "\n",
    "x = 5\n",
    "df1 = df.filter(df._1 < x)\n",
    "df1.show()\n",
    "x = 3\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f():\n",
    "    return x/2\n",
    "x = 5\n",
    "df1 = df.select(df._1 * f() * f() / 3 + 1)\n",
    "df1.show()\n",
    "x = 3\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "x = 5\n",
    "a = rdd.filter(lambda z: z < x)\n",
    "print a.take(10)\n",
    "x = 3\n",
    "print a.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += 1\n",
    "\n",
    "df.foreach(increment_counter)\n",
    "\n",
    "print counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 = [1, 1, 1, 2, 2, 2, 3, 3, 3, 4]\n",
    "data2 = [2, 2, 3, 4, 5, 3, 1, 1, 2, 3]\n",
    "df = spark.createDataFrame(zip(data1, data2))\n",
    "print df.rdd.getNumPartitions()\n",
    "print df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df.repartition(6, df._2)\n",
    "print df1.rdd.glom().collect()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}